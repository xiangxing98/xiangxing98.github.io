---
title: "R_Text_Mining_Chinese_Word_Segmentation"
author: "Stone_Hou"
date: "2017年7月17日"
output:
  html_notebook:
    theme: readable
    toc: yes
    toc_depth: 4
---

# 中文分词学习笔记 R_Text_Mining_Chinese_Word_Segmentation

> [http://blog.fens.me/r-word-jiebar/](http://blog.fens.me/r-word-jiebar/)

> [jiebaR中文分词快速入门](http://blog.csdn.net/songzhilian22/article/details/49250489)

文本挖掘是数据挖掘中一个非常重要的部分，有非常广阔的使用场景，比如我们可以对新闻事件进行分析，了解国家大事；也可以对微博信息进行分析，通过社交舆情看看大家的关注点。通过文本挖掘找到文章中的隐藏信息，对文章的结构进行分析，判断是不是同一个作者写文章；同时可以对邮件分析，结合bayes算法判断哪些是垃圾邮件，哪些是有用的邮件。

文本挖掘的第一步，就是要进行分词，分词将直接影响文本挖掘的效果。R语言在分词方面有很好的支持，接下来就给大家介绍一个不错的R语言中文分词包“结巴分词”(jiebaR)。

## 1. 简介与安装-JiebaR Introduction & Installation

### 1.1 简介

JiebaR is a package for Chinese text segmentation, keyword extraction and speech tagging. 结巴分词(jiebaR)，是一款高效的R语言中文分词包，底层使用的是C++，通过Rcpp进行调用很高效。结巴分词基于MIT协议，就是免费和开源的，感谢国人作者的给力支持，让R的可以方便的处理中文文本。

官方Github的地址：[https://github.com/qinwf/jiebaR](https://github.com/qinwf/jiebaR)

```{r install jiebaR}
# Install the latest development version from GitHub:
# 同时还可以通过Github安装[开发版]，
# 建议使用 gcc >= 4.9 编译，Windows需要安装 Rtools ：
library(devtools)
install_github("qinwf/jiebaRD")
install_github("qinwf/jiebaR")
library("jiebaR")
# devtools::install_github("qinwf/jiebaR")

# Install from CRAN:
install.packages("jiebaR")
library("jiebaR")
```

### 1.2 上手案例

```{r ex01}
# import package in a safe way
if(!suppressWarnings(require('jiebaR'))) {
  install.packages('jiebaR')
  require('jiebaR')
}

#initial 
wk = worker()

wk["jiebaR提供了四种分词模式，可以通过函数worker()来初始化分词引擎，使用函数segment()进行分词。具体使用?worker查看帮助"]
#  [1] "jiebaR"  "提供"    "了"      "四种"   
#  [5] "分"      "词模式"  "可以"    "通过"   
#  [9] "函数"    "worker"  "来"      "初始化" 
# [13] "分词"    "引擎"    "使用"    "函数"   
# [17] "segment" "进行"    "分词"    "具体"   
# [21] "使用"    "worker"  "查看"    "帮助"   

wk["结巴分词(jiebaR)，是一款高效的R语言中文分词包"]
#  [1] "结巴"   "分词"   "jiebaR" "是"     "一款"
#  [6] "高效"   "的"     "R"      "语言"   "中文"
# [11] "分词"   "包"
```

jiebaR提供了3种分词语句的写法，例子上面的用`[]`符号的语法，还可以使用`<=`符合语法，或者使用`segment()`函数。虽然形式不同，但是分词效果是一样的。

```{r ex02 3 syntax}
# 使用<=符号的语法，如下
wk <= '使用小于等于的另一种符合的语法'
# [1] "使用" "小于" "等于" "的"   "另"   "一种"
# [7] "符合" "的"   "语法"

# 使用segment()函数的语法，如下
segment( "使用segment()函数的语法，如下" , wk )
# [1] "使用"    "segment" "函数"    "的"     
# [5] "语法"    "如下"
```

我们也可以直接对文本文件进行分词，在当前目录新建一个文本文件file_segmentation.txt。
```{r file segmentation}
# 在调用worker()函数时，我们实际是在加载jiebaR库的分词引擎。jiebaR库提供了7种分词引擎。
# 
# 混合模型(MixSegment):是四个分词引擎里面分词效果较好的类，结它合使用最大概率法和隐式马尔科夫模型。
# 最大概率法(MPSegment) :负责根据Trie树构建有向无环图和进行动态规划算法，是分词算法的核心。
# 隐式马尔科夫模型(HMMSegment):是根据基于人民日报等语料库构建的HMM模型来进行分词，主要算法思路是根据(B,E,M,S)四个状态来代表每个字的隐藏状态。 HMM模型由dict/hmm_model.utf8提供。分词算法即viterbi算法。
# 索引模型(QuerySegment):先使用混合模型进行切词，再对于切出来的较长的词，枚举句子中所有可能成词的情况，找出词库里存在。
# 标记模型(tag)
# Simhash模型(simhash)
# 关键词模型(keywods)

wk['E:/03-Download/Github/xiangxing98.github.io/R_Learning/file_segmentation.txt']
# "E:/03-Download/Github/xiangxing98.github.io/R_Learning/file_segmentation.segment.2017-07-17_01_42_20.txt"
```



jiebaR提供了四种分词模式，可以通过函数worker()来初始化分词引擎，使用函数segment()进行分词。具体使用?worker查看帮助

```{r load jiebaR package ex1}
# import package in a safe way
if(!suppressWarnings(require('jiebaR'))) {
  install.packages('jiebaR')
  require('jiebaR')
}

text <- '你要明白，这仅仅是一个测试文本'
mixseg <- worker() #使用默认参数，混合模型（MixSegment）

segment(text, mixseg)
#等价于mixseg[text]
#也等价于mixseg <= text
# [1] "你"   "要"   "明白" "这"   "仅仅" "是"   "一个" "测试" "文本"

# 直接输入mixseg命令，可以查看此worker的配置
mixseg
# Worker Type:  Jieba Segment
# 
# Default Method  :  mix
# Detect Encoding :  TRUE
# Default Encoding:  UTF-8
# Keep Symbols    :  FALSE
# Output Path     :  
# Write File      :  TRUE
# By Lines        :  FALSE
# Max Word Length :  20
# Max Read Lines  :  1e+05
# 
# Fixed Model Components:  
# 
# $dict
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/jieba.dict.utf8"
# 
# $user
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/user.dict.utf8"
# 
# $hmm
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/hmm_model.utf8"
# 
# $stop_word
# NULL
# 
# $user_weight
# [1] "max"
# 
# $timestamp
# [1] 1500174243
# 
# $default $detect $encoding $symbol $output $write $lines $bylines can be reset.

# another example 
library(jiebaR)
#  接受默认参数，建立分词引擎 
mixseg = worker()
# 相当于：
# worker( type = "mix", dict = "inst/dict/jieba.dict.utf8",
#         hmm  = "inst/dict/hmm_model.utf8",    # HMM模型数据
#         user = "inst/dict/user.dict.utf8")    # 用户自定义词库
# Initialize jiebaR worker 初始化worker

# This function can initialize jiebaR workers. 
# You can initialize different kinds of workers including mix, mp, hmm, query, tag, simhash, and keywords.

mixseg <= "广东省深圳市联通"    # <= 分词运算符
# 相当于segment函数，看起来还是用segment函数顺眼一些

segment(code= "广东省深圳市联通" , jiebar = mixseg)
# code A Chinese sentence or the path of a text file.
# jiebar jiebaR Worker

# 分词结果
# [1] "广东省" "深圳市" "联通" 

mixseg <= "你知道我不知道"
# [1] "你"   "知道" "我"   "不"   "知道"

mixseg <= "我昨天参加了同学婚礼"
# [1] "我"   "昨天" "参加" "了"   "同学" "婚礼"

mixseg <= "你知道吗我昨天参加了同学的婚礼"
 # [1] "你"   "知道" "吗"   "我"   "昨天" "参加" "了"   "同学"
 # [9] "的"   "婚礼"
# 呵呵：分词结果还算不错

```

可以通过R语言常用的 `$` 符号重设一些worker的参数设置，如 WorkerName `$symbol = T`，在输出中保留标点符号。

一些参数在初始化的时候已经确定，无法修改， 可以通过`WorkerName$PrivateVarible`来获得这些信息。

```{r file segmentation ex1}
#自动判断输入文件编码模式，默认文件输出在同目录下。
segment('D:/test.txt', mixseg) 
#等价于mixseg['D:/test.txt']
#也等价于mixseg <= 'D:/test.txt'

mixseg <= 'E:\\03-Download\\Github\\红楼梦.txt'
#GB2312 CODING, NO OUTPUT

mixseg <= 'E:/03-Download/Github/红楼梦-UTF-8.txt'
#STILL NO OUTPUT

segment('你今天要到哪里去？', mixseg) 

segment('E:/03-Download/Github/红楼梦-UTF-8.txt', mixseg)
# [1] "E:/03-Download/Github/红楼梦-UTF-8.segment.2017-07-16_11_42_23.txt"

```

## 2. 四种分词算法-four kinds of segmentation model

### 2.1 最大概率法（MPSegment）： 

负责根据Trie树构建有向无环图和进行动态规划算法，是分词算法的核心。

Maximum probability segmentation model uses Trie tree to construct a directed acyclic graph and uses dynamic programming algorithm. It is the core segmentation algorithm. dict and user should be provided when initializing jiebaR worker.

```{r Maximum probability segmentation model ex1}
text <- '你要明白，这仅仅是一个测试文本'
mpseg <- worker('mp') #最大概率法（MPSegment）
mpseg[text]
# [1] "你"   "要"   "明白" "这"   "仅仅" "是"   "一个" "测试" "文本"
```

### 2.2 隐式马尔科夫模型（HMMSegment）： 

是根据基于人民日报等语料库构建的HMM模型来进行分词，主要算法思路是根据(B,E,M,S)四个状态来代表每个字的隐藏状态。 HMM模型由dict/hmm_model.utf8提供。分词算法即viterbi算法。

Hidden Markov Model uses HMM model to determine status set and observed set of words. The default HMM model is based on People's Daily language library. hmm should be provided when initializing jiebaR worker.

```{r Hidden Markov segmentation Model ex1}
text <- '你要明白，这仅仅是一个测试文本'
hmmseg <- worker('hmm') #隐式马尔科夫模型（HMMSegment）
hmmseg[text]
# [1] "你"   "要"   "明白" "这仅" "仅"   "是"   "一个" "测试" "文本"
```

### 2.3 混合模型（MixSegment）： 

是四个分词引擎里面分词效果较好的类，结它合使用最大概率法和隐式马尔科夫模型。

MixSegment model uses both Maximum probability segmentation model and Hidden Markov Model to construct segmentation. dict, hmm and user should be provided when initializing jiebaR worker.

```{r MixSegment segmentation Model ex1}
text <- '你要明白，这仅仅是一个测试文本'
mixseg <- worker('mix') #混合模型（MixSegment）
mixseg[text]
# [1] "你"   "要"   "明白" "这"   "仅仅" "是"   "一个" "测试" "文本"
```

### 2.4 索引模型（QuerySegment）： 

先使用混合模型进行切词，再对于切出来的较长的词，枚举句子中所有可能成词的情况，找出词库里存在。

QuerySegment model uses MixSegment to construct segmentation and then enumerates all the possible long words in the dictionary. dict, hmm and qmax should be provided when initializing jiebaR worker.

There is a symbol <= for this function.

```{r QuerySegment segmentation Model ex1}
text <- '你要明白，这仅仅是一个测试文本'
queryseg <- worker('query') #索引模型（QuerySegment）
queryseg[text]
# [1] "你"   "要"   "明白" "这"   "仅仅" "是"   "一个" "测试" "文本"

```

### 2.5 worker()函数的定义

```{r worker()}
worker(type = "mix", dict = DICTPATH, hmm = HMMPATH, user = USERPATH,
  idf = IDFPATH, stop_word = STOPPATH, write = T, qmax = 20, topn = 5,
  encoding = "UTF-8", detect = T, symbol = F, lines = 1e+05,
  output = NULL, bylines = F, user_weight = "max")

# 参数列表：
# 
# type, 引擎类型
# dict, 系统词典
# hmm, HMM模型路径
# user, 用户词典
# idf, IDF词典
# stop_word, 关键词用停止词库
# write, 是否将文件分词结果写入文件，默认FALSE
# qmax, 最大成词的字符数，默认20个字符
# topn, 关键词数,默认5个
# encoding, 输入文件的编码，默认UTF-8
# detect, 是否编码检查，默认TRUE
# symbol, 是否保留符号，默认FALSE
# lines, 每次读取文件的最大行数，用于控制读取文件的长度。大文件则会分次读取。
# output, 输出路径
# bylines, 按行输出
# user_weight, 用户权重
```

我们在调用worker()时，就加载了分词引擎，可以打印出来，查看分词的引擎的配置。

```{r Default Method}
wk = worker()
wk
# Worker Type:  Jieba Segment
# 
# Default Method  :  mix
# Detect Encoding :  TRUE
# Default Encoding:  UTF-8
# Keep Symbols    :  FALSE
# Output Path     :  
# Write File      :  TRUE
# By Lines        :  FALSE
# Max Word Length :  20
# Max Read Lines  :  1e+05
# 
# Fixed Model Components:  
# 
# $dict
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/jieba.dict.utf8"
# 
# $user
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/user.dict.utf8"
# 
# $hmm
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/hmm_model.utf8"
# 
# $stop_word
# NULL
# 
# $user_weight
# [1] "max"
# 
# $timestamp
# [1] 1500226127
# 
# $default $detect $encoding $symbol $output $write $lines $bylines can be reset.
```

如果我们想改变分词引擎的配置项，可以在调用`worker()`创建分词引擎时，也可以通过`wk$XX`来进行设置。如果想了解`wk`是什么类型的对象，我们通过`pryr`包的otype的函数来检查`wk`对象的类型。关于`pryr`包的详细使用，请参考文章[撬动R内核的高级工具包pryr](http://blog.fens.me/r-pryr/)
```{r pryr package}
# 加载 pryr包
library(pryr)
otype(wk)  # 面向对象的类型检查
# [1] "S3"

class(wk)  # 查看class是属性
# [1] "jiebar"  "segment" "jieba" 
```




##  3. 标注词性-Speech Tagging

Speech Tagging worker uses MixSegment model to cut word and tag each word after segmentation using labels compatible with ictclas.

dict, hmm and user should be provided when initializing jiebaR worker.

可以使用`<=.tagger` 或者`tag` 来进行分词和词性标注，词性标注使用混合模型模型分词，标注采用和 `ictclas` 兼容的标记法。

jiebaR包关于词典词性标记，采用ictclas的标记方法。ICTCLAS 汉语词性标注集。

代码	名称	帮助记忆的诠释
Ag	形语素	形容词性语素。形容词代码为a，语素代码ｇ前面置以A。
a	形容词	取英语形容词adjective的第1个字母。
ad	副形词	直接作状语的形容词。形容词代码a和副词代码d并在一起。
an	名形词	具有名词功能的形容词。形容词代码a和名词代码n并在一起。
b	区别词	取汉字"别"的声母。
c	连词	取英语连词conjunction的第1个字母。
Dg	副语素	副词性语素。副词代码为d，语素代码ｇ前面置以D。
d	副词	取adverb的第2个字母，因其第1个字母已用于形容词。
e	叹词	取英语叹词exclamation的第1个字母。
f	方位词	取汉字"方"的声母。
g	语素	绝大多数语素都能作为合成词的"词根"，取汉字"根"的声母。
h	前接成分	取英语head的第1个字母。
i	成语	取英语成语idiom的第1个字母。
j	简称略语	取汉字"简"的声母。
k	后接成分	
l	习用语	习用语尚未成为成语，有点"临时性"，取"临"的声母。
m	数词	取英语numeral的第3个字母，n，u已有他用。
Ng	名语素	名词性语素。名词代码为n，语素代码ｇ前面置以N。
n	名词	取英语名词noun的第1个字母。
nr	人名	名词代码n和"人(ren)"的声母并在一起。
ns	地名	名词代码n和处所词代码s并在一起。
nt	机构团体	"团"的声母为t，名词代码n和t并在一起。
nz	其他专名	"专"的声母的第1个字母为z，名词代码n和z并在一起。
o	拟声词	取英语拟声词onomatopoeia的第1个字母。
p	介词	取英语介词prepositional的第1个字母。
q	量词	取英语quantity的第1个字母。
r	代词	取英语代词pronoun的第2个字母,因p已用于介词。
s	处所词	取英语space的第1个字母。
Tg	时语素	时间词性语素。时间词代码为t,在语素的代码g前面置以T。
t	时间词	取英语time的第1个字母。
u	助词	取英语助词auxiliary 的第2个字母,因a已用于形容词。
Vg	动语素	动词性语素。动词代码为v。在语素的代码g前面置以V。
v	动词	取英语动词verb的第一个字母。
vd	副动词	直接作状语的动词。动词和副词的代码并在一起。
vn	名动词	指具有名词功能的动词。动词和名词的代码并在一起。
w	标点符号	
x	非语素字	非语素字只是一个符号，字母x通常用于代表未知数、符号。
y	语气词	取汉字"语"的声母。
z	状态词	取汉字"状"的声母的前一个字母。


```{r Speech Tagging worker ex1}
text <- '你要明白，这仅仅是一个测试文本'
tagseg <- worker('tag')

tagseg[text]
  #    r      v     nr      r      d      v      m     vn      n 
  # "你"   "要" "明白"   "这" "仅仅"   "是" "一个" "测试" "文本" 

# same results
tagging(text, tagseg)
  #    r      v     nr      r      d      v      m     vn      n 
  # "你"   "要" "明白"   "这" "仅仅"   "是" "一个" "测试" "文本" 
# another example
cutter = worker(type = "tag")
cutter_words <- cutter <= "我爱北京天安门"
cutter_words
#        r        v       ns       ns 
#      "我"     "爱"     "北京"     "天安门" 
# # "我"  反身代词； "爱" 动词； "北京" 名词


```

## 4. 关键词提取-Keyword Extraction

关键词提取是文本处理非常重要的一个环节，一个经典算法是TF-IDF算法。其中，TF（Term Frequency）代表词频，IDF（Inverse Document Frequency）表示逆文档频率。如果某个词在文章中多次出现，而且不是停止词，那么它很可能就反应了这段文章的特性，这就是我们要找的关键词。再通过IDF来算出每个词的权重，不常见的词出现的频率越高，则权重越大。计算TF-IDF的公式为：`TF-IDF = TF(词频) * 逆文档频率(IDF)`

对文档中每个词计算TF-IDF的值，把结果从大到小排序，就得到了这篇文档的关键性排序列表。关于IF-IDF的解释，参考了文章[TF-IDF与余弦相似性的应用（一）：自动提取关键词。](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)

jiebaR包的关键词提取提取的实现，也是使用了TF-IDF的算法。在安装目录中的idf.utf8文件，为IDF的语料库。查看idf.utf8内容。

```{r}
show_dictpath()
# D:/Program Files/R/R-3.3.3/library/jiebaRD/dict
dir(show_dictpath())
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict"
#  [1] "backup.rda"      "hmm_model.utf8"  "hmm_model.zip"  
#  [4] "idf.utf8"        "idf.zip"         "jieba.dict.utf8"
#  [7] "jieba.dict.zip"  "model.rda"       "README.md"      
# [10] "stop_words.utf8" "user.dict.utf8" 

scan(file="D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/idf.utf8",
      what='character',nlines=50,sep='\n',
      encoding='UTF-8',fileEncoding='UTF-8')
# invalid input found on input connection 'D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/idf.utf8'Read 1 item

#delete fileEncoding='UTF-8'
scan(file="D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/idf.utf8",
      what='character',nlines=50,sep='\n',
      encoding='UTF-8')
```

idf.utf8文件每一行有2列，第一列是词项，第二列为权重。然后，我通过计算文档的词频(TF)，与语料库的IDF值相乘，就可以得到TF-IDF值，从而提取文档的关键词。

比如，我们对下面的文本内容进行关键词的提取。
```{r Keyword Extraction ex1}
wk = worker()
segment<-wk["R的极客理想系列文章，涵盖了R的思想，使用，工具，创新等的一系列要点，以我个人的学习和体验去诠释R的强大。"]

# 计算词频
freq(segment)

# 取TF-IDF的前5的关键词
keys = worker("keywords",topn=5)

# 计算关键词
vector_keywords(segment,keys)
# 11.7392 8.97342 8.23425  8.2137 7.43298 
 # "极客"  "诠释"  "要点"  "涵盖"  "体验" 

```
使用jiebaR包处理分词确实简单，几行的代码就能实现分词的各种算法操作。有了这个工具，我们就可以文档中，发现各种语言规则进行文本挖掘了

Keyword Extraction worker uses MixSegment model to cut word and use TF-IDF algorithm to find the keywords. dict ,hmm, idf, stop_word and topn should be provided when initializing jiebaR worker.

关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径，使用方法与分词类似。topn参数为关键词的个数。

```{r keys ex1}
text <- '你要明白，这仅仅是一个测试文本'

keys = worker('keywords', topn = 2) #参数topn表示提取排在最前的关键词个数
keys <= text
# 8.94485 7.14724 
#  "文本"  "测试" 

#同样的，也可以对文件进行关键词提取
keys <= "一个文件路径.txt"
keys <= "E:/03-Download/Github/红楼梦-UTF-8.txt" 
# 26138.9 9266.45 
#  "宝玉"  "贾母" 

## another example
cutter = worker(type = "keywords", topn = 2)
cutter_words <- cutter <= "我爱北京天安门"
cutter_words
  # 8.9954   4.6674 
  # "天安门"   "北京"
# 根据IDF算法，"我" "爱" 的逆文本频率过低，topn=2，就被过滤掉了

```

## 5. simhash计算-分词-关键词提取-文本去重

Simhash worker uses the keyword extraction worker to find the keywords and uses simhash algorithm to compute simhash. dict hmm, idf and stop_word should be provided when initializing jiebaR worker.

对中文文档计算出对应的simhash值。simhash是谷歌用来进行文本去重的算法，现在广泛应用在文本处理中。Simhash引擎先进行分词和关键词提取，后计算Simhash值和海明距离。

```{r simhasher ex1}
text <- '你要明白，这仅仅是一个测试文本'

simhasher = worker("simhash", topn = 2)
simhasher <= text
# $simhash
# [1] "10014870797707624170"
# 
# $keyword
# 8.94485 7.14724 
#  "文本"  "测试" 

#看看红楼梦的关键词是什么
# "宝玉"   "贾母"   "凤姐" "王夫人" "老太太"   "太太" 
simhasher = worker("simhash", topn = 10)
simhasher <= "E:/03-Download/Github/红楼梦-UTF-8.txt"
# $simhash
# [1] "14760579568630162737"
# 
# $keyword
#  26138.9  9266.45  8269.94  7909.09  7020.27  5631.21 
#   "宝玉"   "贾母"   "凤姐" "王夫人" "老太太"   "太太" 
#   5603.1  5512.09  5451.25  5062.17 
#   "姑娘"   "贾琏"   "奶奶"   "众人" 

# another example
cutter = worker(type = "simhash", topn = 2)
cutter_words <- cutter <= "我爱北京天安门"
cutter_words
# $simhash
# [1] "4352745221754575559"
# 
# $keyword
#   8.9954   4.6674 
# "天安门"   "北京" 

```

## 6. 快速模式-Not Recomended

无需使用函数`worker()`，使用默认参数启动引擎，并立即进行分词。使用`qseg(quick segmentation)`，使用默认分词模式，自动建立分词引擎，类似于`ggplot2`包里面的`qplot`函数。

```{r quick segmentation qseg ex1}
text <- '你要明白，这仅仅是一个测试文本'
qseg <= text
# Quick mode is depreciated, and is scheduled to be remove in v0.11.0. If you want to keep this feature, please submit a issue on GitHub page to let me know.
# [1] "你"   "要"   "明白" "这"   "仅仅" "是"   "一个" "测试" "文本"

worker('mix') #查看worker('mix')参数配置
qseg #查看qseg参数配置，与上面一样都得到以下结果
# 实际上，第一次运行时，会启动默认引擎 quick_worker，相当于先运行了一遍代码：qseg = worker('mix')

# Worker Type:  Jieba Segment
# 
# Default Method  :  mix
# Detect Encoding :  TRUE
# Default Encoding:  UTF-8
# Keep Symbols    :  FALSE
# Output Path     :  
# Write File      :  TRUE
# By Lines        :  FALSE
# Max Word Length :  20
# Max Read Lines  :  1e+05
# 
# Fixed Model Components:  
# 
# $dict
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/jieba.dict.utf8"
# 
# $user
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/user.dict.utf8"
# 
# $hmm
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/hmm_model.utf8"
# 
# $stop_word
# NULL
# 
# $user_weight
# [1] "max"
# 
# $timestamp
# [1] 1500192552
# 
# $default $detect $encoding $symbol $output $write $lines $bylines can be reset.
```


可以通过qseg$重设模型参数，重设模型参数将会修改以后每次默认启动的默认参数；

如果只是想临时修改模型参数，可以使用非快速模式的修改方式quick_worker$。

```{r qseg ex2}
# 重设模型参数的同时，重新启动引擎；下次重新启动包时，现有的设置不会改变。
qseg$type = "mp" 

# 临时修改，下次重新启动包时，会恢复原来的默认设置。
quick_worker$detect = T 

# 获得当前快速模式的默认参数
get_qsegmodel()         
```

## 7. 加载词库

常用的分词包有两种加载词库的方法，就是加载包时读取默认的词典和数据模型，或者在分词前加载词典和模型数据。在早期的版本中，jiebaR也使用过这两种方式进行加载。

第一种方式，就像一个铁笼子，加载包时一次性加载了词库，封装在一起。

第二种方式灵活，可以动态地加载词库和模型数据，适时进行修改，但是每次分词前，加载词库都十分耗费时间，对于小的任务不合适。

有了Rcpp Modules，jiebaR可以把C++中的分词类映射到R语言中的RC类，把这样原本C++中静态的类的操作，带到了R里面，可以动态地运行。在jiebaR里，你可以动态地生成分词器，使用不同的分词器，对不同类型的文本进行操作，分词就像切菜时选不同的菜刀一样。

对于分词的结果好坏的关键因素是词典，jiebaR默认有配置标准的词典。对于我们的使用来说，不同行业或不同的文字类型，最好用专门的分词词典。在jiebaR中通过`show_dictpath()`函数可以查看默认的标准词典，可以通过上一小节介绍的配置项，来指定我们自己的词典。日常对话的常用词典，比如搜狗输入法的词库。

`library(jiebaR)`加载包时，没有启动任何分词引擎，启动引擎很简单，就是一句赋值语句就可以了。`cutter <- worker()`

软件默认设定非常重要，`jiebaR`默认参数为绝大多数任务调整到了最好的状态（哈哈，我的自我感觉）。初始化分词简单，分词就更简单了。为了让大家少一些待在电脑前的时间，多一些配家人和朋友的时间，少敲一些键盘，`jiebaR`重载了`<=`这个不太常用的符号，当然还有`==`,你在项目README里可以看到。分词就是一个类似赋值的过程，足够简单粗暴：
```{r segmentation ex1}
# start segmentation engine
# import package in a safe way
if(!suppressWarnings(require('jiebaR'))) {
  install.packages('jiebaR')
  require('jiebaR')
}

cutter <- worker()

# cut what
cutter <= "江州市长江大桥，参加了长江大桥的通车仪式。"
# perfect results
# [1] "江州"     "市长"     "江大桥"   "参加"     "了"      
# [6] "长江大桥" "的"       "通车"     "仪式"   

# 或者Pipe一个文件路径
cutter <= "weibo.txt"

```

当然，如果你喜欢打字，也可以使用`segment()`函数。正如之前说的，可以同时初始化和使用多个分词器。可以添加一些参数来初始化，可用参数列表很长很长，但是一般你不会全用到它们，具体可以参考帮助文档`?worker()`:
```{r user defined dictionary ex1}
cutter2 <- worker( user = "e:/Path for User defined Dictionary") ### 初始化第二个引擎

ShowDictPath() 
 ### 可以显示默认词典路径


# 查看默认的词库位置
show_dictpath()
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict"

# 查看目录
dir(show_dictpath())
# [1] "D:/Program Files/R/R-3.3.3/library/jiebaRD/dict"
#  [1] "backup.rda"      "hmm_model.utf8"  "hmm_model.zip"  
#  [4] "idf.utf8"        "idf.zip"         "jieba.dict.utf8"
#  [7] "jieba.dict.zip"  "model.rda"       "README.md"      
# [10] "stop_words.utf8" "user.dict.utf8" 

# 看到词典目录中，包括了多个文件。
# 
# jieba.dict.utf8, 系统词典文件，最大概率法，utf8编码的
# hmm_model.utf8, 系统词典文件，隐式马尔科夫模型，utf8编码的
# user.dict.utf8, 用户词典文件，utf8编码的
# stop_words.utf8，停止词文件，utf8编码的
# idf.utf8，IDF语料库，utf8编码的
# jieba.dict.zip，jieba.dict.utf8的压缩包
# hmm_model.zip，hmm_model.utf8的压缩包
# idf.zip，idf.utf8的压缩包
# backup.rda，无注释
# model.rda，无注释
# README.md，说明文件
```

打开系统词典文件jieba.dict.utf8，并打印前50行
```{r jieba.dict.utf8}
#delete fileEncoding='UTF-8'
scan(file="D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/jieba.dict.utf8",
           what=character(),nlines=50,sep='\n',
           encoding='utf-8')
# 系统词典每一行都有三列，并以空格分割，第一列为词项，第二列为词频，第三列为词性标记。

#delete fileEncoding='UTF-8'
scan(file="D:/Program Files/R/R-3.3.3/library/jiebaRD/dict/user.dict.utf8",
      what=character(),nlines=50,sep='\n',
      encoding='utf-8')
# 用户词典第一行有二列，，第一列为词项，第二列为词性标记，没有词频的列。用户词典默认词频为系统词库中的最大词频。
```


这时R的环境里同时有两个加载了不同词库的分词引擎。

如果需要了解这两个不同的引擎的区别只需要print一下就可以了。

每个worker都有一些参数设置，如`cutter`中的`$detect`参数决定了引擎是否自动判断输入文件的编码，在引擎加载时可以通过`worker(detect = F )`进行参数设置，也可以在加载后通过`cutter$detect = F`进行设置。其实 `worker()`函数返回的是一个环境`(environment)`，里面封装了真正的分词引擎，你可以通过`cutter$worker`来查看真正的“引擎”。`cutter$worker <pointer: 0x0805b990>`

`cutter$worker`和`cutter`都是环境，在传递时是传址，而不是传值，效率是比较高的。jiebaR的分词速度是其他R语言分词包的5-20倍。

分词结束后，对于不需要的引擎只需要用rm()进行删除，R有自动的垃圾回收机制，为你解决内存管理的后顾之忧。

分词已经分好，统计分析才是最重要的任务。剃刀已经磨砺，接下来就可以用R来处理中文字符了。

```{r}
mixseg2 = worker(type  = "mix", 
                 dict = "/home/rstudio/R/x86_64-pc-linux-gnu-library/3.1/jiebaRD/dict/jieba.dict.utf8", 
                 hmm   = "/home/rstudio/R/x86_64-pc-linux-gnu-library/3.1/jiebaRD/dict/hmm_model.utf8", 
                 user  = "/home/rstudio/R/x86_64-pc-linux-gnu-library/3.1/jiebaRD/dict/user.dict.utf8", 
                 detect=T,  symbol = F, 
                 lines = 1e+05, output = NULL
                 )  
# detect 自动检查文件编码，lines一次读取文件的行数


#修改 user.utf8
# R语言
# R的极客理想
# 大数据
# 数据

#加载user.utf8
# wk = worker(user='user.utf8')

# 输出worker的设置
mixseg2
#输出结果如下：略
```

### 可以自定义用户词库

```{r user dictionary}
install.packages("devtools")
install.packages("stringi")
install.packages("pbapply")
install.packages("Rcpp")
install.packages("RcppProgress")
library(devtools)
install_github("qinwf/cidian")


# decode scel dictionary
decode_scel(scel = "细胞词库路径", output = "输出文件路径", cpp =  TRUE)

decode_scel(scel = "细胞词库路径",output = "输出文件路径",cpp =  FALSE, progress = TRUE)

# 输出调试信息
decode_scel(scel = "细胞词库路径", output = "输出文件路径", cpp = FALSE, progress = TRUE, rdebug = TRUE)

# system dict with frequency
decode_scel("细胞词库路径", output = "输出文件路径", sysdict_freq = 1)

# load/add user dictionary
## 读取用户词典

load_user_dict(filePath = "用户词典路径", default_tag = "默认标记")

## 读取系统词典
load_sys_dict(filePath = "系统词典路径")

## 增加用户词典词

add_user_words(dict = "load_user_dict 读取的词典", words = "UTF-8 编码文本向量", tags = "标记")

## 增加系统词典词

add_sys_words(dict = "load_sys_dict 读取的词典", words = "UTF-8 编码文本向量", freq = "词频", tags = "标记")

## 删除词典词

remove_words(dict = "load_user_dict 或 load_sys_dict 读取的词典", words = "UTF-8 编码文本向量")

## 写入

write_dict(dict = "load_user_dict 或 load_sys_dict 读取的词典", output = "输出路径")

(userd = load_user_dict(jiebaR::USERPATH))

userd = add_user_words(userd, enc2utf8("测试"), "v")

write_dict(userd, jiebaR::USERPATH)

(userd = load_user_dict(jiebaR::USERPATH))
userd = remove_words(userd, enc2utf8(c("测试","蓝翔")))

write_dict(userd, jiebaR::USERPATH)

(userd = load_user_dict(jiebaR::USERPATH))



ShowDictPath()  # 显示词典路径
EditDict()      # 编辑用户词典
?EditDict()     # 打开帮助系统

Usage # 使用方法
edit_dict(name = "user") # 这个方法过时了
EditDict(name = "user") 
Arguments # 参数
# name    
# the name of dictionary including user, system, stop_word.
```

### 搜狗词典

在实际使用中，jiebaR默认提供的用户词典只有5个单词，太简单了，肯定是不够用的。我们可以用搜狗词典，来丰富用户自己的词库。接下来，让我们配置搜狗词典。你需要安装一个搜狗输入法，具体的安装过程不再解释。

我安装的是搜狗五笔输入法，找到搜狗的安装目录，并找到词典文件。我的搜狗词典，在下面的安装位置。

`C:\Program Files (x86)\SogouWBInput\2.1.0.1288\scd\17960.scel`

把17960.scel文件复制到自己的项目目录里，用文本编辑器打开文件，发现是二进制的。那么我需要用工具进行转换，把二进制的词典转成我们可以使用的文本文件。jiebaR包的作者，同时开发了一个`cidian`项目，可以转换搜狗的词典，那么我们只需要安装`cidian`包即可。

安装cidian项目
```{r cidian}
install.packages("devtools")
install.packages("stringi")
install.packages("pbapply")
install.packages("Rcpp")
install.packages("RcppProgress")
library(devtools)
install_github("qinwf/cidian")
library(cidian)

# 转换二进制词典到文本文件。
# 转换
decode_scel(scel = "./17960.scel",cpp = TRUE)
# output file: ./17960.scel_2016-07-21_00_22_11.dict

# 查看生成的词典文件
#delete fileEncoding='UTF-8'
scan(file="./17960.scel_2016-07-21_00_22_11.dict",
      what=character(),nlines=50,sep='\n',
      encoding='utf-8')
```

接下来，直接把搜狗词典配置到我们的分词库中，就可以直接使用了。

把搜狗词典文件改名，从`17960.scel_2016-07-21_00_22_11.dict`到`user.dict.utf8`，然后替换`D:\tool\R-3.2.3\library\jiebaRD\dict`目录下面`的user.dict.utf8`。这样默认的用户词典，就是搜狗词典了。很酷吧！


### 停止词过滤

停止词就是分词过程中，我们不需要作为结果的词，像英文的语句中有很多的a,the,or,and等，中文语言中也有很多，比如 的，地，得，我，你，他。这些词因为使用频率过高，会大量出现在一段文本中，对于分词后的结果，在统计词频的时候会增加很多的噪音，所以我们通常都会将这些词进行过滤。

在jiebaR中，过滤停止词有2种方法，一种是通过配置`stop_word`文件，另一种是使用`filter_segment()`函数。

首先我们先来看，通过配置`stop_word`文件的方法。新建一个`stop_word.txt`文件。
```{r stop_word.txt}
# 我
# 我是


# 加载分词引擎，并配置停止词过滤。
wk = worker(stop_word='stop_word.txt')
segment<-wk["我是《R的极客理想》图书作者"]
segment
# [1] "R"    "的"   "极客" "理想" "图书" "作者"

```

上面的文本，我们把"我是"通过停止词进行了过滤。如果还想过滤“作者”一词，可以动态的调用`filter_segment()`函数
```{r filter_segment}
filter <-c ("作者")
filter_segment(segment,filter)
# [1] "R"    "的"   "极客" "理想" "图书"

```



## 8. 词云图-word cloud

```{r jiebaR & wordcloud}
library(jiebaR)
library(wordcloud2)

#读入数据分隔符是‘\n’，字符编码是‘UTF-8’，what=''表示以字符串类型读入
f <- scan('E:/03-Download/Github/红楼梦-UTF-8.txt',sep='\n',what='',encoding="UTF-8")

# 初始化
mixseg <- worker()
segment(head(f), mixseg)
#  [1] "红楼梦"   "曹雪芹"   "著"       "手机"     "电子书"  
#  [6] "大学生"   "小说网"   "Txt"      "版"       "阅读"    
# [11] "阅读"     "作品"     "更"       "多"       "请"      
# [16] "访问"     "http"     "www"      "dxsxs"    "com"     
# [21] "书籍"     "介绍"     "中国"     "古代"     "四大名著"
# [26] "之一"     "章节"     "内容"     "开始"     "上卷"    
# [31] "第一回"   "甄士隐"   "梦幻"     "识通灵"   "　"      
# [36] "贾雨村"   "风尘"     "怀"       "闺秀"    

# Data Manupulation
# 全文分词
seg <- segment(f, mixseg)
# 查看分词后的向量的长度
length(seg)
# [1] 470995

# 查看分词后的向量的前50的词频统计
sort(table(seg), decreasing = T)[1:50]
#     了     的     我     他     你     也     是     说     　 
#  17775  13505   7268   6077   5810   5745   5624   5372   5158 
#     又     道   宝玉     着     来     这     人     不     去 
#   4955   4425   3748   3651   3254   3002   3001   2956   2950 
#     便     在     有     都     就     叫     那     听     贾 
#   2927   2784   2597   2533   2398   1899   1818   1748   1646 
#   什么     见     等     要     还   一个   笑道     好     呢 
#   1613   1581   1565   1521   1505   1450   1447   1442   1412 
#     只     和   我们   那里     上     到     儿     倒     因 
#   1295   1222   1221   1174   1128   1122   1107   1085   1042 
# 王夫人     才   你们   如今     们 
#   1011   1010   1009    999    985

#单个的字太多，没有意义

seg <- seg[nchar(seg)>1 & nchar(seg) < 7] #去除字符长度小于2的词语,&<7
sort(table(seg), decreasing = T)[1:50]
# 宝玉   什么   一个   笑道   我们   那里 王夫人   你们   如今 
#   3748   1613   1450   1447   1221   1174   1011   1009    999 
#   说道   知道 老太太   起来   姑娘   这里   出来   他们   众人 
#    973    967    966    944    941    935    922    895    870 
#   自己   一面   太太   只见   怎么   奶奶   两个   没有   不是 
#    836    829    825    789    777    772    769    761    729 
#   不知   这个   听见   这样   贾母   进来   咱们   告诉   就是 
#    708    697    689    646    632    632    605    602    601 
#   东西   平儿   回来   只是   大家   老爷   只得   丫头   这些 
#    599    588    566    544    543    540    531    509    504 
#   不敢   凤姐   出去 凤姐儿   所以 
#    496    492    483    470    466 

#seg <- table(seg) 统计词频

seg <- seg[!grepl('[0-9]+',names(seg))] #去除数字
length(seg) #查看处理完后剩余的词数


#降序排序，并提取出现次数最多的前100个词语, 查看100个词频最高的
Top100 <- sort(table(seg), decreasing = TRUE)[1:100];Top100
# 宝玉   什么   一个   笑道   我们   那里 王夫人   你们   如今 
#   3748   1613   1450   1447   1221   1174   1011   1009    999 
#   说道   知道 老太太   起来   姑娘   这里   出来   他们   众人 
#    973    967    966    944    941    935    922    895    870 
#   自己   一面   太太   只见   怎么   奶奶   两个   没有   不是 
#    836    829    825    789    777    772    769    761    729 
#   不知   这个   听见   这样   贾母   进来   咱们   告诉   就是 
#    708    697    689    646    632    632    605    602    601 
#   东西   平儿   回来   只是   大家   老爷   只得   丫头   这些 
#    599    588    566    544    543    540    531    509    504 
#   不敢   凤姐   出去 凤姐儿   所以 薛姨妈   不过   的话   不好 
#    496    492    483    470    466    453    448    445    444 
#   姐姐   探春   鸳鸯   一时   不能   过来   去了   心里   二爷 
#    442    432    425    421    420    420    404    402    399 
#   如此   今日   银子   几个   答应   二人   宝钗   还有   只管 
#    376    370    366    358    358    356    356    350    343 
#   这么   黛玉   说话   一回   晴雯   那边   湘云   外头   这话 
#    343    342    340    338    332    330    324    317    317 
#   贾琏   打发   自然   袭人   今儿   罢了   屋里 刘姥姥   那些 
#    313    310    306    298    297    296    295    293    293 
#   听说 小丫头 邢夫人 林黛玉   如何   问道   看见   妹妹   人家 
#    290    287    284    280    279    277    274    272    269 
#   不用 
#    264 



# Top100做词云
jpeg("E:/03-Download/Github/RedChamber_Top100.jpg", width = 500, height = 500)
par(bg = "black")
wordcloud2(Top100, size =1, color = 'random-light', shape = 'cardioid')
# ?wordcloud2()
dev.off()

```
### wordcloud2 example demo

```{r wordcloud2 ex2}
library("wordcloud2")
wordcloud2(demoFreq, size = 1,shape = 'star')

wordcloud2(demoFreq, size = 2, minRotation = -pi/2, maxRotation = -pi/2)

wordcloud2(demoFreq, size = 2, minRotation = -pi/6, maxRotation = -pi/6,
  rotateRatio = 1)

wordcloud2(demoFreqC, size = 2, fontFamily = "微软雅黑",
           color = "random-light", backgroundColor = "grey")

## Sys.setlocale("LC_CTYPE","eng")
wordcloud2(demoFreqC, size = 2, fontFamily = "微软雅黑",
           color = "random-light", backgroundColor = "grey")
```



##参考资料：

* [红楼梦搜狗词库](http://pinyin.sogou.com/dict/search/search_list/%BA%EC%C2%A5%C3%CE/normal/1)

* [jiebaR中文分词快速入门](http://blog.csdn.net/songzhilian22/article/details/49250489)

* [http://blog.fens.me/r-word-jiebar/](http://blog.fens.me/r-word-jiebar/)

* [jiebaR](https://cran.r-project.org/web/packages/jiebaR/index.html)

* [jiebaR Tutouria-PDF](https://cran.r-project.org/web/packages/jiebaR/jiebaR.pdf)

* [R news and tutorials R bloggers](https://www.r-bloggers.com/)

* [CRAN cloud](https://cloud.r-project.org/)

* [Garrett & Hadley: R For Data Science](http://r4ds.had.co.nz/)

* [Write HTML, PDF, ePub, and Kindle books with R Markdown](https://bookdown.org/)

* [The tidyverse style guide](http://style.tidyverse.org/index.html)

* [http://tidyverse.org/](http://tidyverse.org/)


