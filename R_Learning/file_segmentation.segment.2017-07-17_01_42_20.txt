在 调用 worker 函数 时 我们 实际 是 在 加载 jiebaR 库 的 分词 引擎 jiebaR 库 提供 了 7 种 分词 引擎 混合 模型 MixSegment 是 四个 分词 引擎 里面 分词 效果 较 好 的 类 结它 合 使用 最大 概率法 和 隐式 马尔科夫 模型 最大 概率法 MPSegment 负责 根据 Trie 树 构建 有 向 无 环图 和 进行 动态 规划 算法 是 分词 算法 的 核心 隐式 马尔科夫 模型 HMMSegment 是 根据 基于 人民日报 等 语料库 构建 的 HMM 模型 来 进行 分词 主要 算法 思路 是 根据 B E M S 四个 状态 来 代表 每个 字 的 隐藏 状态 HMM 模型 由 dict hmm model utf8 提供 分词 算法 即 viterbi 算法 索引 模型 QuerySegment 先 使用 混合 模型 进行 切词 再 对于 切出来 的 较 长 的 词 枚举 句子 中 所有 可能 成词 的 情况 找出 词 库里 存在 标记 模型 tag Simhash 模型 simhash 关键词 模型 keywods